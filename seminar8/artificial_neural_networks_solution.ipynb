{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adasegroup/ML2023_seminars/blob/master/seminar10/artificial_neural_networks_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 10: Artificial neural networks, MLP, PyTorch\n",
    "\n",
    "In this seminar we will get familiar with artificial neural networks and one of the most common frameworks to work with them --- PyTorch.\n",
    "Our humble ultimate goal will be to program and train an MLP to classify MNIST digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/adasegroup/ML2020_seminars/raw/master/seminar12/img/pic1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An artificial neural network --- ANN or just NN, is simply a composition of functions with free 'trainable' parameters, or 'weights'.\n",
    "In this seminar, we will be talking about NNs that are feedforward, i.e don't have cyclic compositions of the functions,\n",
    "and we will be talking about them in the context of approximation tasks.\n",
    "\n",
    "More formally, for any functions $f_i$\n",
    "$$ f_1(x|w_1),\\quad f_2(x, f_1(x)|w_2),\\quad f_n(x, f_1(x), \\ldots, f_{n-1}(\\ldots)|w_n),$$\n",
    "$F(x) = f_n(\\ldots)$ is a feedforward neural network with weights $w_1,\\ldots,w_n$,\n",
    "and we will want it to approximate something, e.g another function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, let's say that we want to find a linear fit to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_n = 100\n",
    "x = np.random.rand(points_n) * 10 - 5\n",
    "y = x * 3 + 4\n",
    "y += np.random.randn(points_n) / 2\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call function $F(x) = ax + b$ a neural network with weights $a$ and $b$, and train this neural network, which means find such values of $a$ and $b$ that $F(x)$ fits the data above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNs are trained with backpropagation algorithm, which is basically\n",
    "1. Calculate the value of the loss function, that represents how far from desired the output of the network is\n",
    "2. Calculate the derivative of the loss w.r.t to each trainable parameter of the network\n",
    "3. Update the values of the parameters with some rule using these derivatives\n",
    "4. Repeat steps 1-3 for N iterations, or until the loss is low enough, or something else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.\n",
    "Train $F(x) = ax + b$ to fit the data above. Use plain Python or NumPy, not PyTorch.\n",
    "\n",
    "More formally, we have $x_i,\\,y_i,\\,i=1,\\ldots,n_\\mathrm{pts}$\n",
    "1. Use mean squared error as the loss function $L(a, b) = \\frac{1}{n_\\mathrm{pts}}\\sum_i (F(x_i | a, b) - y_i)^2$\n",
    "2. Calculate the derivatives of the loss w.r.t to $a$ and $b$\n",
    "3. Update the weights of the network using Gradient Descent\n",
    "$$ a_\\mathrm{new} = a_\\mathrm{old} - \\lambda\\partial_a L(a_\\mathrm{old}, b_\\mathrm{old}),\\quad b_\\mathrm{new} = b_\\mathrm{old} - \\lambda\\partial_b L(a_\\mathrm{old}, b_\\mathrm{old}), $$\n",
    "where $\\lambda$ is a numerical paramer called \"learning rate\"\n",
    "4. Repeat steps 1-3 until you are satisfied with the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the data again\n",
    "np.random.seed(0)\n",
    "points_n = 100\n",
    "x = np.random.rand(points_n) * 10 - 5\n",
    "x.sort()  # sort points, merely for correct visualization\n",
    "y = x * 3 + 4\n",
    "y += np.random.randn(points_n) / 2\n",
    "\n",
    "# define the network and initial values of the parameters\n",
    "a = 1\n",
    "b = 2\n",
    "\n",
    "def f(x):\n",
    "    return x * a + b\n",
    "\n",
    "# visualization code\n",
    "fig = plt.figure()\n",
    "\n",
    "def visualize(fig, x, y, nn_outputs):\n",
    "    fig.clear()\n",
    "    plt.title(f'Iteration {iter_i}')\n",
    "    plt.scatter(x, y, c='tab:blue')\n",
    "    plt.plot(x, nn_outputs, c='tab:orange')\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "\n",
    "# train\n",
    "learning_rate = 1e-1\n",
    "iterations_n = 30\n",
    "\n",
    "for iter_i in range(iterations_n):\n",
    "    nn_outputs = f(x)\n",
    "    \n",
    "    # calculate the derivatives\n",
    "    dl_a = 2. / points_n * np.sum((nn_outputs - y) * x)\n",
    "    dl_b = 2. / points_n * np.sum((nn_outputs - y) * 1)\n",
    "    \n",
    "    # update the parameters\n",
    "    a = a - dl_a * learning_rate\n",
    "    b = b - dl_b * learning_rate\n",
    "\n",
    "    visualize(fig, x, y, nn_outputs)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.\n",
    "Now try to fit this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "points_n = 100\n",
    "x = np.random.rand(points_n) * np.pi*3 - np.pi*1.5\n",
    "x.sort()\n",
    "y = np.sin(x)\n",
    "y += np.random.randn(points_n) / 10\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a bit more complex function $F(x) = f_3(f_2(f_1(x)))$\n",
    "\n",
    "$$f_1(x) = \\mathbf{a}_1\\,x + \\mathbf{b}_1,\\quad \\mathbf{a}_1,\\,\\mathbf{b}_1\\in\\mathbb{R}^{80}, \\\\\n",
    "f_2(\\mathbf{x})_i = \\max(0, x_i),\\quad i = 1, \\ldots, 80, \\\\\n",
    "f_3(\\mathbf{x}) = \\mathbf{a}_2\\cdot\\mathbf{x} + b_2,\\quad \\mathbf{a}_2\\in\\mathbb{R}^{80},\\,b_2\\in\\mathbb{R}.\n",
    "$$\n",
    "\n",
    "The training algorithm is the same, the main difference is calculation of derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network and initial values of the parameters\n",
    "a1 = np.random.rand(80)\n",
    "b1 = np.random.rand(80)\n",
    "a2 = np.random.rand(80)\n",
    "b2 = 1\n",
    "\n",
    "def f(x):\n",
    "    f1_out = x[..., None] * a1 + b1  # [points_n, 80]\n",
    "    f2_out = np.maximum(f1_out, 0)\n",
    "    f3_out = np.matmul(f2_out, a2) + b2\n",
    "    return f3_out\n",
    "\n",
    "# train\n",
    "learning_rate = 1e-3\n",
    "iterations_n = 1\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for iter_i in range(iterations_n):\n",
    "    nn_outputs = f(x)\n",
    "    \n",
    "    # calculate the derivatives\n",
    "    # Your code here\n",
    "    \n",
    "    # update the parameters\n",
    "    # Your code here\n",
    "\n",
    "    if iter_i % 100 == 0:\n",
    "        visualize(fig, x, y, nn_outputs)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that easy, right?\n",
    "\n",
    "The key here is to use chain rule for calculation of gradients\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{a}_1} = \\sum_i \\frac{\\partial L}{\\partial F(x_i)} \\frac{\\partial F(x_i)}{\\partial \\mathbf{a}_1},\\quad\n",
    "\\frac{\\partial F(x_i)}{\\partial \\mathbf{a}_1} = \\frac{\\partial F(x_i)}{\\partial f_3(x_i)}\\frac{\\partial f_3(x_i)}{\\partial f_2(x_i)}\\frac{\\partial f_2(x_i)}{\\partial f_1(x_i)}\\frac{\\partial f_1(x_i)}{\\partial \\mathbf{a}_1},\\quad \\text{etc},$$\n",
    "where\n",
    "$$L = \\frac{1}{n_\\mathrm{pts}}\\sum_i (F(x_i) - y_i)^2,\\\\\n",
    "F(x) = f_3(f_2(f_1(x))).$$\n",
    "\n",
    "The neural networks used for real-world tasks are composed of many more functions, and manually programming this each time would be impractical.\n",
    "Deep learning framewors like PyTorch provide a lot of preprogrammed functionality like this.\n",
    "\n",
    "Let's look at how we can use PyTorch to solve Exercise 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan of the seminar\n",
    "1. Intro to the seminar\n",
    "2. Intro to PyTorch\n",
    "3. Automatic differentiation in PyTorch\n",
    "4. PyTorch Modules\n",
    "5. Batching\n",
    "6. Optimization\n",
    "7. Putting everything together\n",
    "8. Classifying MNIST with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Intro to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is mostly like NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in numpy\n",
    "a = np.random.rand(100)\n",
    "b = np.empty(100)\n",
    "\n",
    "np.maximum(a, np.array([0.]))\n",
    "np.matmul(a, b)\n",
    "\n",
    "np.asarray([1,2,3]).astype(np.float32)\n",
    "\n",
    "# in pytorch\n",
    "a = torch.rand(100)\n",
    "b = torch.empty(100)\n",
    "\n",
    "torch.max(a, torch.tensor([0.]))\n",
    "torch.matmul(a, b)\n",
    "\n",
    "torch.as_tensor([1,2,3]).type(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etc.\n",
    "It is useful to look through the complete list of what there is \\[[1](https://pytorch.org/docs/stable/torch.html#math-operations), [2](https://pytorch.org/docs/stable/nn.functional.html)\\].\n",
    "For this course generally this is not necessary, but if you're going to use PyTorch for your research or the course project, there are plenty of convenient functions useful to be aware of.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neat feature of PyTorch is that it is allows to accelerate computations using GPU transparently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.random.rand(1000, 1000).astype(np.float32)\n",
    "m2 = np.random.rand(1000, 1000).astype(np.float32)\n",
    "for _ in trange(1000):\n",
    "    m1 @ m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = torch.rand(1000, 1000, dtype=torch.float32, device='cuda')\n",
    "m2 = torch.rand(1000, 1000, dtype=torch.float32, device='cuda')\n",
    "for _ in trange(10_000):\n",
    "    m1 @ m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatic differentiation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch can automatically calculate derivatives of predefined functions and their compositions.\n",
    "To indicate that we want to calculate derivative of something w.r.t some tensor, we need to set the `requires_grad` flag of this tensor to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.])\n",
    "x.requires_grad_()\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate $\\partial_x y(x)$ we need to call `y.backward()` (notice, not `y.backward(x)`). The value of the derivative is then stored in `x.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "print(x.grad, '==', torch.cos(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for composition of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)\n",
    "y = x ** 2 + 1\n",
    "y.backward()\n",
    "print(x.grad, '==', 2 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for multivariate functions, e.g $y = \\sum_i x_i a_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(10, requires_grad=True)\n",
    "a = torch.arange(len(x), dtype=x.dtype)\n",
    "y = torch.sum(x * a)\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tensor already has `grad` attribute, e.g from the previous call to `backward`, subsequent calls will add to the value of this attribute.\n",
    "So, if you, e.g, `backward()` in a loop, you need to explicitely set `grad` to zero each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.], requires_grad=True)\n",
    "x.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = x * 2\n",
    "b = x * 3\n",
    "y = a + b  # == x * 5\n",
    "# in this case y.backward() is the same as\n",
    "a.backward()\n",
    "b.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.], requires_grad=True)\n",
    "print(x.grad is None)\n",
    "a = x * 2\n",
    "b = x * 3\n",
    "a.backward()\n",
    "print(x.grad)\n",
    "b.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.], requires_grad=True)\n",
    "for i in range(2):\n",
    "    y = x * 2\n",
    "    y.backward()\n",
    "    print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.], requires_grad=True)\n",
    "for i in range(2):\n",
    "    y = x * 2\n",
    "    y.backward()\n",
    "    print(x.grad)\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensors that don't require grad are transparently converted to numpy arrays.\n",
    "To convert a tensor that requires grad, we need to call its `detach` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones([10], requires_grad=True)\n",
    "# plt.plot(x)  # doesn't work\n",
    "plt.plot(x.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch offers a [variety of modules](https://pytorch.org/docs/stable/nn.html) representing different functions with already built-in trainable parameters.\n",
    "We can implement our $f_1$, $f_2$ and $f_3$ with predefined modules, that are called [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) and [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU).\n",
    "\n",
    "$$f_1(x) = \\mathbf{a}_1\\,x + \\mathbf{b}_1,\\quad \\mathbf{a}_1,\\,\\mathbf{b}_1\\in\\mathbb{R}^{80}, \\\\\n",
    "f_2(\\mathbf{x})_i = \\max(0, x_i),\\quad i = 1, \\ldots, 80, \\\\\n",
    "f_3(\\mathbf{x}) = \\mathbf{a}_2\\cdot\\mathbf{x} + b_2,\\quad \\mathbf{a}_2\\in\\mathbb{R}^{80},\\,b_2\\in\\mathbb{R}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 = torch.rand(80, requires_grad=True)\n",
    "# b1 = torch.rand(80, requires_grad=True)\n",
    "# a2 = torch.rand(80, requires_grad=True)\n",
    "# b2 = torch.rand(1, requires_grad=True)\n",
    "# def f(x):\n",
    "#     f1_out = x[..., None] * a1 + b1\n",
    "#     f2_out = torch.max(f1_out, torch.tensor([0.]))\n",
    "#     f3_out = torch.matmul(f2_out, a2) + b2\n",
    "#     return f3_out\n",
    "\n",
    "\n",
    "f1 = torch.nn.Linear(1, 80, bias=True)\n",
    "f2 = torch.nn.ReLU()\n",
    "f3 = torch.nn.Linear(80, 1, bias=True)\n",
    "\n",
    "def f(x):\n",
    "    return f3(f2(f1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = list(f1.parameters())\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another convenient way of stacking the modules is with [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(x):\n",
    "#     return f3(f2(f1(x)))\n",
    "\n",
    "f = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 80, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(80, 1, bias=True)\n",
    ")\n",
    "len(list(f.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another set of useful modules is [Loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "For Exercise 2 we will need [MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)\n",
    "\n",
    "$$L = \\frac{1}{n_\\mathrm{pts}}\\sum_i (F(x_i) - y_i)^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "nn_outputs = torch.ones(100) * 2\n",
    "y = torch.zeros(100)\n",
    "loss_function(nn_outputs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inteface of PyTorch modules allows for some batch dimensions in the input data, along which the module acts independently and in parallel.\n",
    "For instance, [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) and [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) allow for any number of batch dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = torch.nn.Linear(1, 80, bias=True)\n",
    "\n",
    "x = torch.rand(1)\n",
    "f1(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 5, 1)\n",
    "f1(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "torch.manual_seed(0)\n",
    "points_n = 100\n",
    "x = torch.rand(points_n, dtype=torch.float32) * np.pi * 3 - np.pi * 1.5\n",
    "x = x.sort()[0]\n",
    "y = torch.sin(x)\n",
    "y += torch.randn(points_n) / 10\n",
    "plt.scatter(x, y)\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of points `points_n` corresponds to the batch dimension, since we want to calculate the output of the network for each point independently\n",
    "\n",
    "$$L = \\frac{1}{n_\\mathrm{pts}}\\sum_i (F(x_i) - y_i)^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[..., None]  # add data dimension\n",
    "y = y[..., None]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has implementions of different [optimization algorithms](https://pytorch.org/docs/stable/optim.html), or optimizers, for updating the weights of the neural network.\n",
    "Today we use [Gradient Descent](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD).\n",
    "\n",
    "The way use an optimizer is to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize it, setting the learning rate and the optimized parameters\n",
    "optimizer = torch.optim.SGD(f.parameters(), lr=1e-3)\n",
    "\n",
    "# 2. At each training step we need to zero the gradients of the parameters --- optimizer can do this for us\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 3. Update the parameters\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Putting everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try doing the Exercise 2 with PyTorch.\n",
    "\n",
    "### Exercise 2.5.\n",
    "Fit the data with the function $F(x) = f_3(f_2(f_1(x)))$\n",
    "\n",
    "$$f_1(x) = \\mathbf{a}_1\\,x + \\mathbf{b}_1,\\quad \\mathbf{a}_1,\\,\\mathbf{b}_1\\in\\mathbb{R}^{80}, \\\\\n",
    "f_2(\\mathbf{x})_i = \\max(0, x_i),\\quad i = 1, \\ldots, 80, \\\\\n",
    "f_3(\\mathbf{x}) = \\mathbf{a}_2\\cdot\\mathbf{x} + b_2,\\quad \\mathbf{a}_2\\in\\mathbb{R}^{80},\\,b_2\\in\\mathbb{R}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "points_n = 100\n",
    "x = torch.rand(points_n, dtype=torch.float32) * np.pi * 3 - np.pi * 1.5\n",
    "x = x.sort()[0]\n",
    "y = torch.sin(x)\n",
    "y += torch.randn(points_n) / 10\n",
    "plt.scatter(x, y)\n",
    "\n",
    "x = x[..., None]\n",
    "y = y[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_size = 80\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, hidden_dim_size, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_dim_size, 1, bias=True)\n",
    ")\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "fig = plt.figure()\n",
    "\n",
    "for iter_i in range(10_000):\n",
    "    nn_outputs = net(x)\n",
    "    loss = loss_function(nn_outputs, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter_i % 100 == 0:\n",
    "        visualize(fig, x, y, nn_outputs.detach())\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Classifying MNIST with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a dataset of images with handwritten digits.\n",
    "Each image is labeled with the corresponding number.\n",
    "We'll play around with a toy version of this dataset which has ~2k 8x8 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = load_digits(return_X_y=True)\n",
    "\n",
    "grid = np.pad(images[:20].reshape(20, 8, 8), [[0, 0], [3, 3], [3, 3]]).reshape(2, 10, 14, 14).swapaxes(1, 2).reshape(28, -1)\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.imshow(grid, cmap='gray_r')\n",
    "targets[:20].reshape(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the data for further work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train, images_test, targets_train, targets_test = train_test_split(images, targets, random_state=0)\n",
    "images_train = torch.from_numpy(images_train.astype(np.float32))\n",
    "images_test = torch.from_numpy(images_test.astype(np.float32))\n",
    "targets_train = torch.from_numpy(targets_train)\n",
    "targets_test = torch.from_numpy(targets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will unroll each image into an array of 64 values and will train our $F(x)$ from above to predict an array of 10 values, each corresponding to probability of the image depicting the respective digit.\n",
    "\n",
    "More formally, $F(\\mathbf{x}) = f_3(f_2(f_1(\\mathbf{x}))),\\quad \\mathbf{x}\\in\\mathbb{R}^{64}$,\n",
    "$$f_1(\\mathbf{x}) = A_1\\cdot \\mathbf{x} + \\mathbf{b}_1,\\quad A_1\\in\\mathbb{R}^{80\\times 64},\\quad \\mathbf{b}_1\\in\\mathbb{R}^{80}, \\\\\n",
    "f_2(\\mathbf{x})_i = \\max(0, x_i),\\quad i = 1, \\ldots, 80, \\\\\n",
    "f_3(\\mathbf{x}) = A_2\\cdot\\mathbf{x} + \\mathbf{b}_2,\\quad A_2\\in\\mathbb{R}^{10\\times 80},\\,\\mathbf{b}_2\\in\\mathbb{R}^{10}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_size = 80\n",
    "input_size = 8 * 8\n",
    "number_of_classes = 10\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_size, hidden_dim_size, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_dim_size, number_of_classes, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to get probablities $p_i$ of different digits out of the network,\n",
    "we will pass the output of the network to softmax function\n",
    "$$\\mathrm{SoftMax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "to enforce the properties of probabilities,\n",
    "i.e $0 \\le p_i \\le 1$, $\\sum_i p_i = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_outputs = net(images_train[:2])\n",
    "net_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.all((0 <= net_outputs) & (net_outputs <= 1)))\n",
    "print(net_outputs.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "probabilities = softmax(net_outputs)\n",
    "\n",
    "print(torch.all((0 <= probabilities) & (probabilities <= 1)))\n",
    "print(probabilities.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then just take the index of maximal predicted probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probabilies, ids_of_max_probabilities = probabilities.max(dim=-1)\n",
    "ids_of_max_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of MSELoss, that we used for regression, we will use [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss), suitable for multi-class classification.\n",
    "As the input it takes raw outputs of the network and the target class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_function(net_outputs, targets_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "Let's put everything together and try to train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network\n",
    "hidden_dim_size = 80\n",
    "input_size = 8 * 8\n",
    "number_of_classes = 10\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_size, hidden_dim_size, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_dim_size, number_of_classes, bias=True)\n",
    ")\n",
    "\n",
    "# use the same optimizer as before\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\n",
    "\n",
    "# use the cross entropy loss function\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# for monitoring we'll use confusion matrices on the train and test sets\n",
    "fig, [ax_train, ax_test] = plt.subplots(1, 2, figsize=[10, 5])\n",
    "\n",
    "def visualize():\n",
    "    ax_train.clear()\n",
    "    ax_train.set_title(f'Iteration {iter_i}, train')\n",
    "    ax_train.matshow(confusion_matrix(targets_train, get_predicted_classes(images_train)))\n",
    "    ax_test.set_title(f'test')\n",
    "    ax_test.matshow(confusion_matrix(targets_test, get_predicted_classes(images_test)))\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "    \n",
    "def get_predicted_classes(images):\n",
    "    return softmax(net(images).detach()).max(dim=-1)[1]\n",
    "\n",
    "iters_n = 1000\n",
    "for iter_i in range(iters_n):\n",
    "    nn_outputs = net(images_train)\n",
    "    loss = loss_function(nn_outputs, targets_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter_i % 10 == 0:\n",
    "        visualize()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
